{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the code to get the paper citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     def get_full_text(self):\n",
    "#         self.html = urllib.request.urlopen(self.paper_url)# grab html\n",
    "#         self.soup = bs.BeautifulSoup(self.html,'lxml')# trun pretty \n",
    "#         ## extract\n",
    "#         self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "#         self.cite_flag = True;\n",
    "#         if self.citations is None:\n",
    "#             print('cant get citations for ',self.paper_url) \n",
    "#         elif not self.citations: # NN\n",
    "#             self.citations = self.soup.find('ol', {'class':'references'}).select('li[id]') # get citations\n",
    "#             if self.citations is None:\n",
    "#                 print('cant get citations for ',self.paper_url) \n",
    "#             elif not a.citations:\n",
    "#                 print('cant get citations for ',self.paper_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# links_output = []\n",
    "# for link in a.soup.find_all('a'):\n",
    "#     links_output.append(link.get('href'))\n",
    "    \n",
    "# for i in range(0,len(links_output)): \n",
    "#     if links_output[i] is None:\n",
    "#         continue\n",
    "#     else:\n",
    "#         if '.full' in links_output[i]:\n",
    "#             if 'http:' in links_output[i]:\n",
    "#                 master_link = links_output[i]\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   self.html = urllib.request.urlopen(self.paper_url)# grab html\n",
    "#         self.soup = bs.BeautifulSoup(self.html,'lxml')# trun pretty \n",
    "#         ## extract\n",
    "#         ## auto detect journal\n",
    "#         if \"Nature\" in self.soup.title.get_text():\n",
    "#             print('Nature Journal')\n",
    "#             self.journal = 'Nature'\n",
    "#             self.citations = self.soup.find('ol', {'class':'references'}).select('li[id]')\n",
    "            \n",
    "#             counter = 0\n",
    "#             prefix = ''#'http://dx.doi.org/'\n",
    "#             links_output = []\n",
    "#             for i in range(len(self.citations[0:])):\n",
    "#                 if self.citations[i].a is None: # if link doesnt exist \n",
    "#                     counter = counter + 1\n",
    "#                     print(i, 'cant find link')\n",
    "#                 elif len(self.citations[i].a['href']) > 90: # if the link is really long \n",
    "#                     print(i, 'long link')\n",
    "# #                     links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "#                 elif 'http://dx.doi.org/' in self.citations[i].a['href']: # normal\n",
    "#                     links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "#                 else: # if its short add prefix\n",
    "#                     links_output.append(re.sub(\"/doifinder/\", '' ,self.citations[i].a['href']))\n",
    "                    \n",
    "#             ## save em to class\n",
    "#             self.links_output = links_output\n",
    "#             self.missing_links = counter\n",
    "#             if len(a.links_output) < 1:\n",
    "#                 counter = 0\n",
    "#                 for link in self.citations:\n",
    "#                     for morelinks in link.find_all('a',href = True):\n",
    "#                         counter = counter + 1\n",
    "#                         print(counter, morelinks['href'])\n",
    "#                         if 'http://dx.doi.org/' in morelinks['href']: # normal\n",
    "#                             links_output.append(re.sub(\"http://dx.doi.org/\", '' ,morelinks['href']))\n",
    "                        \n",
    "#             self.random_ten = np.random.choice(self.links_output,size = 10)\n",
    "            \n",
    "#         ## commented out for simplicity\n",
    "#         elif \"ScienceDirect\" in self.soup.title.get_text(): \n",
    "#             print('ScienceDirect',self.soup.title.get_text())\n",
    "            \n",
    "#             prefix = 'https://api.elsevier.com/content/abstract/doi/'\n",
    "#             suffix = '?apiKey=40acd4c83833cb8ead38b50834823fd9&view=REF'\n",
    "#             doi = self.paper_url\n",
    "\n",
    "#             self.link = prefix+doi+suffix\n",
    "#             self.html = urllib.request.urlopen(self.link)# grab html\n",
    "#             self.soup = bs.BeautifulSoup(self.html,'xml')# trun pretty \n",
    "\n",
    "#             self.citations =[]\n",
    "#             for doi in self.soup.find_all('doi'):\n",
    "#                 self.citations.append(doi.contents[0])\n",
    "\n",
    "#             self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "            \n",
    "#         elif \"Science\" in self.soup.title.get_text(): # science\n",
    "#             print('Science',self.soup.title.get_text())\n",
    "            \n",
    "#             print('getting full text','Science Journal')\n",
    "#             self.journal = 'Science'\n",
    "#             self.soup = bs.BeautifulSoup(self.html,'lxml')#        \n",
    "#             self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "#             if self.citations is None:\n",
    "#                 print('actually abstract getting really')\n",
    "#                 links_output = []\n",
    "#                 for link in self.soup.find_all('a'):\n",
    "#                     links_output.append(link.get('href'))\n",
    "\n",
    "#                 for i in range(0,len(links_output)): \n",
    "#                     if links_output[i] is None:\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         if '.full' in links_output[i]:\n",
    "#                             if 'http:' in links_output[i]:\n",
    "#                                 self.master_link = links_output[i]\n",
    "#                                 break\n",
    "#                 self.html = urllib.request.urlopen(self.master_link)# grab html\n",
    "#                 self.soup = bs.BeautifulSoup(self.html,'lxml')#        \n",
    "#                 self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "#                 self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "#             else:\n",
    "#                 print(self.citations)\n",
    "# #                 self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "#         else:\n",
    "#             print(self.soup.title.get_text(),'unsupported journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_links(self):    \n",
    "#         ## get links\n",
    "#         counter = 0\n",
    "#         prefix = ''#'http://dx.doi.org/'\n",
    "#         links_output = [] # create output \n",
    "#         if self.cite_flag == True:\n",
    "#             for i in range(len(self.citations[0:])):\n",
    "#                 if len(self.citations[i].find_all('a')) == 4:\n",
    "#                     links_output.append(self.citations[i].find_all('a')[3]['href'])\n",
    "#                 elif len(self.citations[i].find_all('a')) < 2 :\n",
    "#                     counter = counter + 1\n",
    "#                     print(i,'cant find link')\n",
    "#                 elif 'http://dx.doi.org/' in self.citations[i].find_all('a')[1]['href']:\n",
    "#                     links_output.append(self.citations[i].find_all('a')[1]['href'])\n",
    "#                 else:\n",
    "#                     counter = counter + 1\n",
    "#                     print(i,'cant find link')\n",
    "\n",
    "#         else: ## v1 \n",
    "#             for i in range(len(self.citations[0:])):\n",
    "#                 if self.citations[i].a is None: # if link doesnt exist \n",
    "#                     counter = counter + 1\n",
    "#                     print(i, 'cant find link')\n",
    "#                 elif len(self.citations[i].a['href']) > 120: # if the link is really long \n",
    "#                     links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "#                 elif 'http://dx.doi.org/' in self.citations[i].a['href']: # normal\n",
    "#                     links_output.append(self.citations[i].a['href'])\n",
    "#                 else: # if its short add prefix\n",
    "#                     links_output.append(prefix + re.sub(\"/doifinder/\", '' ,self.citations[i].a['href']))\n",
    "                    \n",
    "#         ## save em to class\n",
    "#         self.links_output = links_output\n",
    "#         self.missing_links = counter\n",
    "#         self.random_ten = np.random.choice(self.links_output,size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class nerdy:\n",
    "    def __init__(self, paper_url):\n",
    "#         if 'http://dx' in paper_url:\n",
    "#             print('a')\n",
    "#             self.paper_url = paper_url\n",
    "#         elif 'http' in paper_url:\n",
    "#             print('b')\n",
    "#             self.paper_url = paper_url\n",
    "#         else:\n",
    "#             print('c')\n",
    "        self.paper_url = 'http://dx.doi.org/' + paper_url\n",
    "                        \n",
    "    def get_full_text(self,layer):\n",
    "        \n",
    "        self.layer = layer\n",
    "        \n",
    "        prefix = 'https://api.elsevier.com/content/abstract/doi/'\n",
    "        suffix = '?apiKey=40acd4c83833cb8ead38b50834823fd9&view=REF'\n",
    "        doi = self.paper_url.replace(\"http://dx.doi.org/\",\"\")\n",
    "        self.link = prefix+doi+suffix\n",
    "        \n",
    "        # added error handeling\n",
    "        req = urllib.request.Request(self.link)\n",
    "        try: urllib.request.urlopen(req)\n",
    "        except urllib.error.URLError as e:\n",
    "            print('link ' + e.reason)\n",
    "        # proceed normally     \n",
    "        self.html = urllib.request.urlopen(self.link)# grab html\n",
    "        self.soup = bs.BeautifulSoup(self.html,'xml')# trun pretty \n",
    "        \n",
    "        self.citations =[]\n",
    "        self.authors = []\n",
    "        self.journal = []\n",
    "\n",
    "        for reference in self.soup.find_all('reference'):\n",
    "            if reference.doi is None: # remove any articles that dont have a doi\n",
    "#                 print('skip')\n",
    "                continue\n",
    "            else:\n",
    "                self.citations.append(reference.doi.contents[0])\n",
    "                self.authors.append(reference.surname.contents[0])\n",
    "                self.journal.append(reference.sourcetitle.contents[0])\n",
    "                \n",
    "        self.article =  pd.DataFrame({\"authors\": self.authors,\"journal\": self.journal, \\\n",
    "                         \"DOI\": self.citations})  # put in dframe\n",
    "#         self.article = self.article.loc[self.article['DOI'] != 'None'] \n",
    "        self.random_ten = self.article.sample(10)\n",
    "        \n",
    "        if self.layer == 0:\n",
    "            self.random_ten['number'] = np.arange(0.1,1.1,0.1)\n",
    "        else:\n",
    "            self.random_ten['number'] = np.arange(0.1+self.layer,1.1+self.layer,0.1)\n",
    "            \n",
    "        self.random_ten['combined'] = self.random_ten[['authors', 'journal','DOI']].apply(lambda x: '. '.join(x), axis=1)\n",
    "        self.random_ten = self.random_ten.reset_index(drop = True)\n",
    "        \n",
    "        if self.layer == 0: # getting zero level things\n",
    "            self.zero_link = prefix+doi+'?apiKey=40acd4c83833cb8ead38b50834823fd9&'+'&view=META'\n",
    "            self.zero_html = urllib.request.urlopen(self.zero_link)# grab html\n",
    "            self.zero_soup = bs.BeautifulSoup(self.zero_html,'xml')# trun pretty \n",
    "            name = self.zero_soup.surname.contents[0]\n",
    "            pub = self.zero_soup.publicationName.contents[0]\n",
    "            self.random_ten.loc[-1]=[doi,name,pub,'0','.'.join((name,pub,doi))]\n",
    "            self.random_ten.index = self.random_ten.index + 1  # shifting index\n",
    "            self.random_ten = self.random_ten.sort() \n",
    "            self.random_ten = pd.DataFrame(self.random_ten)\n",
    "            \n",
    "#         self.candace.append(self.random_ten)\n",
    "        \n",
    "        return(self.random_ten)\n",
    "        \n",
    "                \n",
    "        \n",
    "    def find_childrens_children(self):\n",
    "        child_list = self.random_ten['DOI']\n",
    "        \n",
    "        self.candace = self.random_ten\n",
    "        \n",
    "        for i in range(1,len(child_list)):\n",
    "            child = nerdy(child_list[i])\n",
    "            child_child = self.get_full_text(i)\n",
    "#             print(child.paper_url)\n",
    "#             print(child_child)\n",
    "            print('working on article ', i, 'citations:')\n",
    "            self.candace = self.candace.append(child_child)\n",
    "        self.candace = self.candace.reset_index(drop=True)\n",
    "#             print(self.candace)\n",
    "            \n",
    "#             self.article.append(child_child)        \n",
    "#         # append to list     \n",
    "#         self.citations =[]\n",
    "#         for doi in self.soup.find_all('doi'):\n",
    "#             self.citations.append(doi.contents[0])\n",
    "\n",
    "#         self.random_ten = np.random.choice(self.citations,size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jecd/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:64: FutureWarning: sort(....) is deprecated, use sort_index(.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>number</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1038/nrn3338</td>\n",
       "      <td>Ranganath</td>\n",
       "      <td>Nature Reviews Neuroscience</td>\n",
       "      <td>0</td>\n",
       "      <td>Ranganath.Nature Reviews Neuroscience.10.1038/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1073/pnas.0337195100</td>\n",
       "      <td>Davachi</td>\n",
       "      <td>Proceedings of the National Academy of Science...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Davachi. Proceedings of the National Academy o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1073/pnas.0705273104</td>\n",
       "      <td>Bowles</td>\n",
       "      <td>Proceedings of the National Academy of Science...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Bowles. Proceedings of the National Academy of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1038/35049064</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Nature Reviews Neuroscience</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Brown. Nature Reviews Neuroscience. 10.1038/35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002/cne.20796</td>\n",
       "      <td>Kondo</td>\n",
       "      <td>Journal of Comparative Neurology</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Kondo. Journal of Comparative Neurology. 10.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1523/JNEUROSCI.3763-03.2004</td>\n",
       "      <td>Nemanic</td>\n",
       "      <td>Journal of Neuroscience</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Nemanic. Journal of Neuroscience. 10.1523/JNEU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1002/(SICI)1096-9861(19961125)375:4&lt;552::AI...</td>\n",
       "      <td>Stefanacci</td>\n",
       "      <td>Journal of Comparative Neurology</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Stefanacci. Journal of Comparative Neurology. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1016/S0028-3908(98)00030-6</td>\n",
       "      <td>Xiang</td>\n",
       "      <td>Neuropharmacology</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Xiang. Neuropharmacology. 10.1016/S0028-3908(9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1523/JNEUROSCI.3711-11.2012</td>\n",
       "      <td>Libby</td>\n",
       "      <td>Journal of Neuroscience</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Libby. Journal of Neuroscience. 10.1523/JNEURO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1002/cne.21346</td>\n",
       "      <td>Kobayashi</td>\n",
       "      <td>Journal of Comparative Neurology</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Kobayashi. Journal of Comparative Neurology. 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1002/hipo.22024</td>\n",
       "      <td>Aggleton</td>\n",
       "      <td>Hippocampus</td>\n",
       "      <td>1</td>\n",
       "      <td>Aggleton. Hippocampus. 10.1002/hipo.22024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  DOI     authors  \\\n",
       "0                                     10.1038/nrn3338   Ranganath   \n",
       "1                             10.1073/pnas.0337195100     Davachi   \n",
       "2                             10.1073/pnas.0705273104      Bowles   \n",
       "3                                    10.1038/35049064       Brown   \n",
       "4                                   10.1002/cne.20796       Kondo   \n",
       "5                      10.1523/JNEUROSCI.3763-03.2004     Nemanic   \n",
       "6   10.1002/(SICI)1096-9861(19961125)375:4<552::AI...  Stefanacci   \n",
       "7                       10.1016/S0028-3908(98)00030-6       Xiang   \n",
       "8                      10.1523/JNEUROSCI.3711-11.2012       Libby   \n",
       "9                                   10.1002/cne.21346   Kobayashi   \n",
       "10                                 10.1002/hipo.22024    Aggleton   \n",
       "\n",
       "                                              journal number  \\\n",
       "0                         Nature Reviews Neuroscience      0   \n",
       "1   Proceedings of the National Academy of Science...    0.1   \n",
       "2   Proceedings of the National Academy of Science...    0.2   \n",
       "3                         Nature Reviews Neuroscience    0.3   \n",
       "4                    Journal of Comparative Neurology    0.4   \n",
       "5                             Journal of Neuroscience    0.5   \n",
       "6                    Journal of Comparative Neurology    0.6   \n",
       "7                                   Neuropharmacology    0.7   \n",
       "8                             Journal of Neuroscience    0.8   \n",
       "9                    Journal of Comparative Neurology    0.9   \n",
       "10                                        Hippocampus      1   \n",
       "\n",
       "                                             combined  \n",
       "0   Ranganath.Nature Reviews Neuroscience.10.1038/...  \n",
       "1   Davachi. Proceedings of the National Academy o...  \n",
       "2   Bowles. Proceedings of the National Academy of...  \n",
       "3   Brown. Nature Reviews Neuroscience. 10.1038/35...  \n",
       "4   Kondo. Journal of Comparative Neurology. 10.10...  \n",
       "5   Nemanic. Journal of Neuroscience. 10.1523/JNEU...  \n",
       "6   Stefanacci. Journal of Comparative Neurology. ...  \n",
       "7   Xiang. Neuropharmacology. 10.1016/S0028-3908(9...  \n",
       "8   Libby. Journal of Neuroscience. 10.1523/JNEURO...  \n",
       "9   Kobayashi. Journal of Comparative Neurology. 1...  \n",
       "10          Aggleton. Hippocampus. 10.1002/hipo.22024  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nerdy('10.1038/nrn3338')\n",
    "a.get_full_text(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on article  1 citations:\n",
      "working on article  2 citations:\n",
      "working on article  3 citations:\n",
      "working on article  4 citations:\n",
      "working on article  5 citations:\n",
      "working on article  6 citations:\n",
      "working on article  7 citations:\n",
      "working on article  8 citations:\n",
      "working on article  9 citations:\n",
      "working on article  10 citations:\n"
     ]
    }
   ],
   "source": [
    "a.find_childrens_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0',)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DG = nx.DiGraph() # initialize networkx graph\n",
    "DOI = tuple(a.candace.number[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DG = nx.DiGraph() # initialize networkx graph\n",
    "\n",
    "# this specifies original article from .csv\n",
    "DOI = tuple(a.candace.combined[article.number == 0])\n",
    "\n",
    "# this function gets original data from original article query\n",
    "def get_data(DOI):\n",
    "    # go get data from DOI\n",
    "    data_from_node = {DOI:1}\n",
    "    return data_from_node\n",
    "\n",
    "# this function gets the 10 citations from the original article\n",
    "def find_children(DOI):\n",
    "    # get children DOIs into list from .csv\n",
    "    article_list = tuple(article.combined[article.number <= 1]) # get DOI children\n",
    "    print('Original article [0] citations: \\n', article_list, '\\n')\n",
    "    return (article_list)\n",
    "\n",
    "# this function gets 10 articles from each of the orignal 10 citations\n",
    "def find_childrens_children(DOI):\n",
    "    child_list = find_children(DOI)\n",
    "    result = []\n",
    "    result.append(child_list)\n",
    "    for child in range(1,len(child_list)):\n",
    "        child_child = tuple(article.combined[(article.number > child) & (article.number <= child + 1)])\n",
    "        print('working on article ', child, 'citations:')\n",
    "        print(child_child,'\\n')\n",
    "        result.append(child_child)\n",
    "    return (result)\n",
    "\n",
    "# this function recursively adds the 10 original citations to the original\n",
    "# article \n",
    "def recursive_add(DOI):\n",
    "    if not DG.has_node( DOI ): # if there is no node for this DOI\n",
    "        node_data = get_data(DOI) \n",
    "        DG.add_node(DOI)\n",
    "        child_list = find_children( DOI )\n",
    "        for child in child_list:\n",
    "            recursive_add(DOI)\n",
    "            print( 'adding: '+child )\n",
    "            DG.add_edge(DOI, child)\n",
    "\n",
    "\n",
    "def add_node(DOI):\n",
    "    recursive_add(DOI)\n",
    "    child_list = find_children( DOI )\n",
    "    childrens_children = find_childrens_children(DOI)\n",
    "    for child in range(1,len(child_list)):\n",
    "#             recursive_add(DOI)\n",
    "#             print( 'child loop 1: ', child)\n",
    "#             DG.add_edge(childrens_children[0][0], childrens_children[0][child])\n",
    "#             print( 'original article & citation: ', childrens_children[0][0], '& ', childrens_children[0][child])\n",
    "        for children in range(0,len(child_list)-1):\n",
    "            if child < 10:\n",
    "                DG.add_edge(childrens_children[0][child],childrens_children[child][children])\n",
    "                print( 'citations of citations: ', childrens_children[0][child], '& ', childrens_children[child][children])\n",
    "            elif child == 10:\n",
    "                DG.add_edge(childrens_children[0][child],childrens_children[child][children])\n",
    "                print( 'citations of citations: ', childrens_children[0][child], '& ', childrens_children[child][children])\n",
    "            else: \n",
    "                DG.add_edge(childrens_children[0][child],childrens_children[child+1][0])\n",
    "                print( 'citations of citations: ', childrens_children[0][child], '& ', childrens_children[child+1][0])\n",
    "        \n",
    "            print( 'child loop 2:', children)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
