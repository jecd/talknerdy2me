{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the code to get the paper citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     def get_full_text(self):\n",
    "#         self.html = urllib.request.urlopen(self.paper_url)# grab html\n",
    "#         self.soup = bs.BeautifulSoup(self.html,'lxml')# trun pretty \n",
    "#         ## extract\n",
    "#         self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "#         self.cite_flag = True;\n",
    "#         if self.citations is None:\n",
    "#             print('cant get citations for ',self.paper_url) \n",
    "#         elif not self.citations: # NN\n",
    "#             self.citations = self.soup.find('ol', {'class':'references'}).select('li[id]') # get citations\n",
    "#             if self.citations is None:\n",
    "#                 print('cant get citations for ',self.paper_url) \n",
    "#             elif not a.citations:\n",
    "#                 print('cant get citations for ',self.paper_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# links_output = []\n",
    "# for link in a.soup.find_all('a'):\n",
    "#     links_output.append(link.get('href'))\n",
    "    \n",
    "# for i in range(0,len(links_output)): \n",
    "#     if links_output[i] is None:\n",
    "#         continue\n",
    "#     else:\n",
    "#         if '.full' in links_output[i]:\n",
    "#             if 'http:' in links_output[i]:\n",
    "#                 master_link = links_output[i]\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   self.html = urllib.request.urlopen(self.paper_url)# grab html\n",
    "#         self.soup = bs.BeautifulSoup(self.html,'lxml')# trun pretty \n",
    "#         ## extract\n",
    "#         ## auto detect journal\n",
    "#         if \"Nature\" in self.soup.title.get_text():\n",
    "#             print('Nature Journal')\n",
    "#             self.journal = 'Nature'\n",
    "#             self.citations = self.soup.find('ol', {'class':'references'}).select('li[id]')\n",
    "            \n",
    "#             counter = 0\n",
    "#             prefix = ''#'http://dx.doi.org/'\n",
    "#             links_output = []\n",
    "#             for i in range(len(self.citations[0:])):\n",
    "#                 if self.citations[i].a is None: # if link doesnt exist \n",
    "#                     counter = counter + 1\n",
    "#                     print(i, 'cant find link')\n",
    "#                 elif len(self.citations[i].a['href']) > 90: # if the link is really long \n",
    "#                     print(i, 'long link')\n",
    "# #                     links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "#                 elif 'http://dx.doi.org/' in self.citations[i].a['href']: # normal\n",
    "#                     links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "#                 else: # if its short add prefix\n",
    "#                     links_output.append(re.sub(\"/doifinder/\", '' ,self.citations[i].a['href']))\n",
    "                    \n",
    "#             ## save em to class\n",
    "#             self.links_output = links_output\n",
    "#             self.missing_links = counter\n",
    "#             if len(a.links_output) < 1:\n",
    "#                 counter = 0\n",
    "#                 for link in self.citations:\n",
    "#                     for morelinks in link.find_all('a',href = True):\n",
    "#                         counter = counter + 1\n",
    "#                         print(counter, morelinks['href'])\n",
    "#                         if 'http://dx.doi.org/' in morelinks['href']: # normal\n",
    "#                             links_output.append(re.sub(\"http://dx.doi.org/\", '' ,morelinks['href']))\n",
    "                        \n",
    "#             self.random_ten = np.random.choice(self.links_output,size = 10)\n",
    "            \n",
    "#         ## commented out for simplicity\n",
    "#         elif \"ScienceDirect\" in self.soup.title.get_text(): \n",
    "#             print('ScienceDirect',self.soup.title.get_text())\n",
    "            \n",
    "#             prefix = 'https://api.elsevier.com/content/abstract/doi/'\n",
    "#             suffix = '?apiKey=40acd4c83833cb8ead38b50834823fd9&view=REF'\n",
    "#             doi = self.paper_url\n",
    "\n",
    "#             self.link = prefix+doi+suffix\n",
    "#             self.html = urllib.request.urlopen(self.link)# grab html\n",
    "#             self.soup = bs.BeautifulSoup(self.html,'xml')# trun pretty \n",
    "\n",
    "#             self.citations =[]\n",
    "#             for doi in self.soup.find_all('doi'):\n",
    "#                 self.citations.append(doi.contents[0])\n",
    "\n",
    "#             self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "            \n",
    "#         elif \"Science\" in self.soup.title.get_text(): # science\n",
    "#             print('Science',self.soup.title.get_text())\n",
    "            \n",
    "#             print('getting full text','Science Journal')\n",
    "#             self.journal = 'Science'\n",
    "#             self.soup = bs.BeautifulSoup(self.html,'lxml')#        \n",
    "#             self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "#             if self.citations is None:\n",
    "#                 print('actually abstract getting really')\n",
    "#                 links_output = []\n",
    "#                 for link in self.soup.find_all('a'):\n",
    "#                     links_output.append(link.get('href'))\n",
    "\n",
    "#                 for i in range(0,len(links_output)): \n",
    "#                     if links_output[i] is None:\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         if '.full' in links_output[i]:\n",
    "#                             if 'http:' in links_output[i]:\n",
    "#                                 self.master_link = links_output[i]\n",
    "#                                 break\n",
    "#                 self.html = urllib.request.urlopen(self.master_link)# grab html\n",
    "#                 self.soup = bs.BeautifulSoup(self.html,'lxml')#        \n",
    "#                 self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "#                 self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "#             else:\n",
    "#                 print(self.citations)\n",
    "# #                 self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "#         else:\n",
    "#             print(self.soup.title.get_text(),'unsupported journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_links(self):    \n",
    "#         ## get links\n",
    "#         counter = 0\n",
    "#         prefix = ''#'http://dx.doi.org/'\n",
    "#         links_output = [] # create output \n",
    "#         if self.cite_flag == True:\n",
    "#             for i in range(len(self.citations[0:])):\n",
    "#                 if len(self.citations[i].find_all('a')) == 4:\n",
    "#                     links_output.append(self.citations[i].find_all('a')[3]['href'])\n",
    "#                 elif len(self.citations[i].find_all('a')) < 2 :\n",
    "#                     counter = counter + 1\n",
    "#                     print(i,'cant find link')\n",
    "#                 elif 'http://dx.doi.org/' in self.citations[i].find_all('a')[1]['href']:\n",
    "#                     links_output.append(self.citations[i].find_all('a')[1]['href'])\n",
    "#                 else:\n",
    "#                     counter = counter + 1\n",
    "#                     print(i,'cant find link')\n",
    "\n",
    "#         else: ## v1 \n",
    "#             for i in range(len(self.citations[0:])):\n",
    "#                 if self.citations[i].a is None: # if link doesnt exist \n",
    "#                     counter = counter + 1\n",
    "#                     print(i, 'cant find link')\n",
    "#                 elif len(self.citations[i].a['href']) > 120: # if the link is really long \n",
    "#                     links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "#                 elif 'http://dx.doi.org/' in self.citations[i].a['href']: # normal\n",
    "#                     links_output.append(self.citations[i].a['href'])\n",
    "#                 else: # if its short add prefix\n",
    "#                     links_output.append(prefix + re.sub(\"/doifinder/\", '' ,self.citations[i].a['href']))\n",
    "                    \n",
    "#         ## save em to class\n",
    "#         self.links_output = links_output\n",
    "#         self.missing_links = counter\n",
    "#         self.random_ten = np.random.choice(self.links_output,size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to get j neuroscience to work better\n",
    "class nerdy:\n",
    "    def __init__(self, paper_url):\n",
    "        if 'http://dx' in paper_url:\n",
    "            print('a')\n",
    "            self.paper_url = paper_url\n",
    "        elif 'http' in paper_url:\n",
    "            print('b')\n",
    "            self.paper_url = paper_url\n",
    "        else:\n",
    "            print('c')\n",
    "            self.paper_url = 'http://dx.doi.org/' + paper_url\n",
    "                        \n",
    "    def get_full_text(self,layer):\n",
    "        \n",
    "        self.layer = layer\n",
    "        \n",
    "        prefix = 'https://api.elsevier.com/content/abstract/doi/'\n",
    "        suffix = '?apiKey=40acd4c83833cb8ead38b50834823fd9&view=REF'\n",
    "        doi = self.paper_url.replace(\"http://dx.doi.org/\",\"\")\n",
    "        self.link = prefix+doi+suffix\n",
    "        \n",
    "        # added error handeling\n",
    "        req = urllib.request.Request(self.link)\n",
    "        try: urllib.request.urlopen(req)\n",
    "        except urllib.error.URLError as e:\n",
    "            print('link ' + e.reason)\n",
    "        # proceed notmally     \n",
    "        self.html = urllib.request.urlopen(self.link)# grab html\n",
    "        self.soup = bs.BeautifulSoup(self.html,'xml')# trun pretty \n",
    "        \n",
    "        self.citations =[]\n",
    "        self.authors = []\n",
    "        self.journal = []\n",
    "\n",
    "        for reference in self.soup.find_all('reference'):\n",
    "            if reference.doi is None: # remove any articles that dont have a doi\n",
    "                print('skip')\n",
    "                continue\n",
    "            else:\n",
    "                self.citations.append(reference.doi.contents[0])\n",
    "                self.authors.append(reference.surname.contents[0])\n",
    "                self.journal.append(reference.sourcetitle.contents[0])\n",
    "                \n",
    "        self.article =  pd.DataFrame({\"authors\": self.authors,\"journal\": self.journal, \\\n",
    "                         \"DOI\": self.citations})  # put in dframe\n",
    "#         self.article = self.article.loc[self.article['DOI'] != 'None'] \n",
    "        self.random_ten = self.article.sample(10)\n",
    "        \n",
    "        if self.layer == 0:\n",
    "            self.random_ten['number'] = np.arange(0.1,1.1,0.1)\n",
    "        else:\n",
    "            self.random_ten['number'] = np.arange(0.1+self.layer,1.1+self.layer,0.1)\n",
    "            \n",
    "        self.random_ten['combined'] = self.random_ten[['authors', 'journal','DOI']].apply(lambda x: '. '.join(x), axis=1)\n",
    "        self.random_ten = self.random_ten.reset_index(drop = True)\n",
    "        \n",
    "        if self.layer == 0: # getting zero level things\n",
    "            self.zero_link = prefix+doi+'?apiKey=40acd4c83833cb8ead38b50834823fd9&'+'&view=META'\n",
    "            self.zero_html = urllib.request.urlopen(self.zero_link)# grab html\n",
    "            self.zero_soup = bs.BeautifulSoup(self.zero_html,'xml')# trun pretty \n",
    "            name = self.zero_soup.surname.contents[0]\n",
    "            pub = self.zero_soup.publicationName.contents[0]\n",
    "            self.random_ten.loc[-1]=[doi,name,pub,'0','.'.join((name,pub,doi))]\n",
    "            self.random_ten.index = self.random_ten.index + 1  # shifting index\n",
    "            self.random_ten = self.random_ten.sort() \n",
    "            self.random_ten = pd.DataFrame(self.random_ten)\n",
    "            \n",
    "#         self.candace.append(self.random_ten)\n",
    "        \n",
    "        return(self.random_ten)\n",
    "        \n",
    "                \n",
    "        \n",
    "    def find_childrens_children(self):\n",
    "        child_list = self.random_ten['DOI']\n",
    "        \n",
    "        for idx,child in enumerate(range(1,len(child_list))):\n",
    "            a = self.nerdy(child)\n",
    "            child_child = self.get_full_text(a.paper_url,idx)\n",
    "            print('working on article ', child, 'citations:')\n",
    "#             self.article.append(child_child)        \n",
    "#         # append to list     \n",
    "#         self.citations =[]\n",
    "#         for doi in self.soup.find_all('doi'):\n",
    "#             self.citations.append(doi.contents[0])\n",
    "\n",
    "#         self.random_ten = np.random.choice(self.citations,size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n"
     ]
    }
   ],
   "source": [
    "a = nerdy('10.1038/nrn3338')\n",
    "a.get_full_text(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.find_childrens_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for reference in a.soup.find_all('reference'):\n",
    "    if reference.doi is None:\n",
    "        print('meow')\n",
    "        continue\n",
    "    else:\n",
    "        print(reference.doi.contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.article\n",
    "a.citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.paper_url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
