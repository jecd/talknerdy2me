{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the code to get the paper citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     def get_full_text(self):\n",
    "#         self.html = urllib.request.urlopen(self.paper_url)# grab html\n",
    "#         self.soup = bs.BeautifulSoup(self.html,'lxml')# trun pretty \n",
    "#         ## extract\n",
    "#         self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "#         self.cite_flag = True;\n",
    "#         if self.citations is None:\n",
    "#             print('cant get citations for ',self.paper_url) \n",
    "#         elif not self.citations: # NN\n",
    "#             self.citations = self.soup.find('ol', {'class':'references'}).select('li[id]') # get citations\n",
    "#             if self.citations is None:\n",
    "#                 print('cant get citations for ',self.paper_url) \n",
    "#             elif not a.citations:\n",
    "#                 print('cant get citations for ',self.paper_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# links_output = []\n",
    "# for link in a.soup.find_all('a'):\n",
    "#     links_output.append(link.get('href'))\n",
    "    \n",
    "# for i in range(0,len(links_output)): \n",
    "#     if links_output[i] is None:\n",
    "#         continue\n",
    "#     else:\n",
    "#         if '.full' in links_output[i]:\n",
    "#             if 'http:' in links_output[i]:\n",
    "#                 master_link = links_output[i]\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to get j neuroscience to work better\n",
    "class nerdy:\n",
    "    def __init__(self, paper_url):\n",
    "        if 'http://dx' in paper_url:\n",
    "            print('a')\n",
    "            self.paper_url = paper_url\n",
    "        elif 'http' in paper_url:\n",
    "            print('b')\n",
    "            self.paper_url = paper_url\n",
    "        else:\n",
    "            print('c')\n",
    "            self.paper_url = 'http://dx.doi.org/' + paper_url\n",
    "\n",
    "        \n",
    "    def elsevier_ref(self):\n",
    "        prefix = 'https://api.elsevier.com/content/abstract/doi/'\n",
    "        suffix = '?apiKey=40acd4c83833cb8ead38b50834823fd9&view=REF'\n",
    "        doi = self.paper_url\n",
    "\n",
    "        self.link = prefix+doi+suffix\n",
    "        self.html = urllib.request.urlopen(self.link)# grab html\n",
    "        self.soup = bs.BeautifulSoup(self.html,'xml')# trun pretty \n",
    "        \n",
    "        self.citations =[]\n",
    "        for doi in self.soup.find_all('doi'):\n",
    "            self.citations.append(doi.contents[0])\n",
    "        \n",
    "        self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "                        \n",
    "    def get_full_text(self):\n",
    "        # added error handeling\n",
    "        req = urllib.request.Request(self.paper_url)\n",
    "        try: urllib.request.urlopen(req)\n",
    "        except urllib.error.URLError as e:\n",
    "            print('link ' + e.reason)\n",
    "            return\n",
    "        else:\n",
    "            self.html = urllib.request.urlopen(self.paper_url)\n",
    "        \n",
    "#         self.html = urllib.request.urlopen(self.paper_url)# grab html\n",
    "        self.soup = bs.BeautifulSoup(self.html,'lxml')# trun pretty \n",
    "        ## extract\n",
    "        ## auto detect journal\n",
    "        if \"Nature\" in self.soup.title.get_text():\n",
    "            print('Nature Journal')\n",
    "            self.journal = 'Nature'\n",
    "            self.citations = self.soup.find('ol', {'class':'references'}).select('li[id]')\n",
    "            \n",
    "            counter = 0\n",
    "            prefix = ''#'http://dx.doi.org/'\n",
    "            links_output = []\n",
    "            for i in range(len(self.citations[0:])):\n",
    "                if self.citations[i].a is None: # if link doesnt exist \n",
    "                    counter = counter + 1\n",
    "                    print(i, 'cant find link')\n",
    "                elif len(self.citations[i].a['href']) > 90: # if the link is really long \n",
    "                    print(i, 'long link')\n",
    "#                     links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "                elif 'http://dx.doi.org/' in self.citations[i].a['href']: # normal\n",
    "                    links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "                else: # if its short add prefix\n",
    "                    links_output.append(re.sub(\"/doifinder/\", '' ,self.citations[i].a['href']))\n",
    "                    \n",
    "            ## save em to class\n",
    "            self.links_output = links_output\n",
    "            self.missing_links = counter\n",
    "            self.random_ten = np.random.choice(self.links_output,size = 10)\n",
    "            \n",
    "        elif \"ScienceDirect\" in self.soup.title.get_text(): \n",
    "            print(self.soup.title.get_text())\n",
    "            \n",
    "            prefix = 'https://api.elsevier.com/content/abstract/doi/'\n",
    "            suffix = '?apiKey=40acd4c83833cb8ead38b50834823fd9&view=REF'\n",
    "            doi = self.paper_url\n",
    "\n",
    "            self.link = prefix+doi+suffix\n",
    "            self.html = urllib.request.urlopen(self.link)# grab html\n",
    "            self.soup = bs.BeautifulSoup(self.html,'xml')# trun pretty \n",
    "\n",
    "            self.citations =[]\n",
    "            for doi in self.soup.find_all('doi'):\n",
    "                self.citations.append(doi.contents[0])\n",
    "\n",
    "            self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "            \n",
    "        elif \"Science\" in self.soup.title.get_text(): # science\n",
    "            print(self.soup.title.get_text())\n",
    "            \n",
    "            print('getting full text','Science Journal')\n",
    "            self.journal = 'Science'\n",
    "            self.soup = bs.BeautifulSoup(self.html,'lxml')#        \n",
    "            self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "            if self.citations is None:\n",
    "                print('actually abstract getting really')\n",
    "                links_output = []\n",
    "                for link in self.soup.find_all('a'):\n",
    "                    links_output.append(link.get('href'))\n",
    "\n",
    "                for i in range(0,len(links_output)): \n",
    "                    if links_output[i] is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if '.full' in links_output[i]:\n",
    "                            if 'http:' in links_output[i]:\n",
    "                                self.master_link = links_output[i]\n",
    "                                break\n",
    "                self.html = urllib.request.urlopen(self.master_link)# grab html\n",
    "                self.soup = bs.BeautifulSoup(self.html,'lxml')#        \n",
    "                self.citations = self.soup.find_all('div', attrs={'class':'References'})\n",
    "                self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "            else:\n",
    "                print(self.citations)\n",
    "                self.random_ten = np.random.choice(self.citations,size = 10)\n",
    "        \n",
    "    def get_links(self):    \n",
    "        ## get links\n",
    "        counter = 0\n",
    "        prefix = ''#'http://dx.doi.org/'\n",
    "        links_output = [] # create output \n",
    "        if self.cite_flag == True:\n",
    "            for i in range(len(self.citations[0:])):\n",
    "                if len(self.citations[i].find_all('a')) == 4:\n",
    "                    links_output.append(self.citations[i].find_all('a')[3]['href'])\n",
    "                elif len(self.citations[i].find_all('a')) < 2 :\n",
    "                    counter = counter + 1\n",
    "                    print(i,'cant find link')\n",
    "                elif 'http://dx.doi.org/' in self.citations[i].find_all('a')[1]['href']:\n",
    "                    links_output.append(self.citations[i].find_all('a')[1]['href'])\n",
    "                else:\n",
    "                    counter = counter + 1\n",
    "                    print(i,'cant find link')\n",
    "\n",
    "        else: ## v1 \n",
    "            for i in range(len(self.citations[0:])):\n",
    "                if self.citations[i].a is None: # if link doesnt exist \n",
    "                    counter = counter + 1\n",
    "                    print(i, 'cant find link')\n",
    "                elif len(self.citations[i].a['href']) > 120: # if the link is really long \n",
    "                    links_output.append(re.sub(\"http://dx.doi.org/\", '' ,self.citations[i].a['href']))\n",
    "                elif 'http://dx.doi.org/' in self.citations[i].a['href']: # normal\n",
    "                    links_output.append(self.citations[i].a['href'])\n",
    "                else: # if its short add prefix\n",
    "                    links_output.append(prefix + re.sub(\"/doifinder/\", '' ,self.citations[i].a['href']))\n",
    "                    \n",
    "        ## save em to class\n",
    "        self.links_output = links_output\n",
    "        self.missing_links = counter\n",
    "        self.random_ten = np.random.choice(self.links_output,size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "link Unauthorized\n"
     ]
    }
   ],
   "source": [
    "a = nerdy('http://dx.doi.org/10.1038/35049064')\n",
    "a.get_full_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10.1038/33396', '10.1037//0735-7044.112.6.1291',\n",
       "       '10.1016/S0165-0173(98)00018-6', '10.1016/S0896-6273(00)80214-7',\n",
       "       '10.1073/pnas.93.24.13494', '10.1016/S0896-6273(00)80214-7',\n",
       "       '10.1006/brcg.1994.1040', '10.1037//0735-7044.114.3.459',\n",
       "       '10.1006/nimg.1995.1038', '10.1038/33396'], \n",
       "      dtype='<U63')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "http://dx.doi.org/10.1037//0735-7044.112.6.1291\n",
      "link Requested Range Not Satisfiable\n"
     ]
    }
   ],
   "source": [
    "a=nerdy('10.1037//0735-7044.112.6.1291')\n",
    "print(a.paper_url)\n",
    "a.get_full_text()\n",
    "# html = urllib.request.urlopen(a.paper_url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10.1111/j.1460-9568.2004.03710.x', '10.1016/j.neuron.2009.08.011',\n",
       "       '10.1093/brain/awp090', '10.1037/a0012895',\n",
       "       '10.1162/jocn.2006.18.9.1586', '10.1016/j.neuron.2009.08.011',\n",
       "       '10.1002/hipo.20109', '10.1037/a0012895',\n",
       "       '10.1016/j.neuropsychologia.2010.01.001',\n",
       "       '10.1162/089892904322926692'], \n",
       "      dtype='<U63')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.random_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResultSet' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-c9a9e0eeb073>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcitations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResultSet' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "for link in a.citations:\n",
    "    print(link.find_all('a').select('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nature Journal\n"
     ]
    }
   ],
   "source": [
    "a = nerdy('http://www.nature.com/nrn/journal/v13/n10/full/nrn3338.html')\n",
    "a.get_full_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/doifinder/10.1038/35049064'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.citations[1].a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html=urllib.request.urlopen('http://dx.doi.org/10.1038/35049064')\n",
    "soup = bs.BeautifulSoup(html,'lxml')# trun pretty \n",
    "## extract\n",
    "citations = soup.find_all('ol', attrs={'class':'references'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = nerdy('http://www.nature.com/nrn/journal/v2/n1/full/nrn0101_051a.html')\n",
    "a.get_full_text()\n",
    "a.citations\n",
    "a.citations[1].a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = nerdy('http://www.nature.com/nrn/journal/v13/n10/full/nrn3338.html')\n",
    "a = nerdy('http://www.nature.com/nrn/journal/v13/n10/full/nrn3338.html') # nature neuro\n",
    "a.get_full_text()\n",
    "a.get_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = nerdy('http://www.jneurosci.org/content/32/19/6550')\n",
    "a.get_full_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is where the magic happens\n",
    "def recursive_add(DOI):\n",
    "    if not DG.has_node( DOI ):\n",
    "        node_data = nerdy(DOI)\n",
    "        print(node_data.paper_url)\n",
    "#         node_data.get_full_text()\n",
    "#         node_data.get_links()\n",
    "        node_data.get_full_text()\n",
    "\n",
    "#         node_data.elsevier_ref()    \n",
    "        DG.add_node( DOI)\n",
    "        child_list = node_data.random_ten\n",
    "#         return(child_list)\n",
    "        print(DOI,child_list)\n",
    "#     for child in child_list:\n",
    "#         print(child)\n",
    "#         recursive_add(child)\n",
    "#         print( 'addding: '+child )\n",
    "#         DG.add_edge(DOI, child)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "https://www.nature.com/neuro/journal/vaop/ncurrent/full/nn.4572.html\n",
      "Nature Journal\n",
      "0 long link\n",
      "1 long link\n",
      "2 long link\n",
      "3 long link\n",
      "4 long link\n",
      "5 long link\n",
      "6 long link\n",
      "7 long link\n",
      "8 long link\n",
      "9 long link\n",
      "10 long link\n",
      "11 long link\n",
      "12 long link\n",
      "13 long link\n",
      "14 long link\n",
      "15 long link\n",
      "16 long link\n",
      "17 long link\n",
      "18 long link\n",
      "19 long link\n",
      "20 long link\n",
      "21 long link\n",
      "22 long link\n",
      "23 long link\n",
      "24 long link\n",
      "25 long link\n",
      "26 long link\n",
      "27 long link\n",
      "28 long link\n",
      "29 long link\n",
      "30 long link\n",
      "31 long link\n",
      "32 long link\n",
      "33 long link\n",
      "34 long link\n",
      "35 long link\n",
      "36 long link\n",
      "37 long link\n",
      "38 long link\n",
      "39 long link\n",
      "40 long link\n",
      "41 long link\n",
      "42 long link\n",
      "43 long link\n",
      "44 long link\n",
      "45 long link\n",
      "46 long link\n",
      "https://www.nature.com/neuro/journal/vaop/ncurrent/full/nn.4572.html ['http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998'\n",
      " 'http://www.mathworks.com/matlabcentral/fileexchange/8998']\n"
     ]
    }
   ],
   "source": [
    "DG = nx.DiGraph()\n",
    "recursive_add('https://www.nature.com/neuro/journal/vaop/ncurrent/full/nn.4572.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = nerdy(a.random_ten[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urllib.request.urlopen(b.paper_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.get_full_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# b.get_full_text()\n",
    "b.soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recursive_add(a.random_ten[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
